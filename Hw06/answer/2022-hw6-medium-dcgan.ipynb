{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv6bjjqyGmqV"
   },
   "source": [
    "# Homework 6 - Generative Adversarial Network\n",
    "This is the sample code for hw6 of 2022 Machine Learning course in National Taiwan University. \n",
    "\n",
    "In this sample code, there are 5 sections:\n",
    "1. Environment setting\n",
    "2. Dataset preparation\n",
    "3. Model setting\n",
    "4. Train\n",
    "5. Inference\n",
    "\n",
    "Your goal is to do anime face generation, if you have any question, please discuss at NTU COOL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnp-5lUFLak7"
   },
   "source": [
    "# Environment setting\n",
    "In this section, we will prepare for the dataset and set some environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qhoMUt9LniJ"
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaJRTJEFLrND"
   },
   "outputs": [],
   "source": [
    "# get dataset from huggingface hub\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n",
    "!apt-get install git-lfs\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/datasets/LeoFeng/MLHW_6\n",
    "!unzip ./MLHW_6/faces.zip -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBkkAB9sO3R4"
   },
   "source": [
    "## Other setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxf1TXTLO6Ek"
   },
   "outputs": [],
   "source": [
    "# import module\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# seed setting\n",
    "def same_seeds(seed):\n",
    "    # Python built-in random module\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(2022)\n",
    "workspace_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg2qsevzOeQT"
   },
   "source": [
    "# Dataset preparation\n",
    "In this section, we prepare for the dataset for Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UT6s1x92OudB"
   },
   "source": [
    "## Create dataset for Pytorch\n",
    "\n",
    "In order to unified image information, we use the transform function to:\n",
    "1. Resize image to 64x64\n",
    "2. Normalize the image\n",
    "\n",
    "This CrypkoDataset class will be use in Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MsHqaglOywi"
   },
   "outputs": [],
   "source": [
    "# prepare for CrypkoDataset\n",
    "\n",
    "class CrypkoDataset(Dataset):\n",
    "    def __init__(self, fnames, transform):\n",
    "        self.transform = transform\n",
    "        self.fnames = fnames\n",
    "        self.num_samples = len(self.fnames)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.fnames[idx]\n",
    "        img = torchvision.io.read_image(fname)\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def get_dataset(root):\n",
    "    fnames = glob.glob(os.path.join(root, '*'))\n",
    "    compose = [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ]\n",
    "    transform = transforms.Compose(compose)\n",
    "    dataset = CrypkoDataset(fnames, transform)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPMZTwAiQSnx"
   },
   "source": [
    "## Show the image\n",
    "Show some sample in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rX5-Q71TOyy4",
    "outputId": "5ccce544-982c-4635-c88a-3a0290985669"
   },
   "outputs": [],
   "source": [
    "temp_dataset = get_dataset(os.path.join(workspace_dir, 'faces'))\n",
    "\n",
    "images = [temp_dataset[i] for i in range(4)]\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgV-jpcfQwEM"
   },
   "source": [
    "# Model setting\n",
    "In this section, we will create models and trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY4rAlw8RNhG"
   },
   "source": [
    "## Create model\n",
    "In this section, we will create models for Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dfregFtRVGo"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (batch, in_dim)\n",
    "    Output shape: (batch, 3, 64, 64)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, feature_dim=64):\n",
    "        super().__init__()\n",
    "    \n",
    "        #input: (batch, 100)\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(feature_dim * 8 * 4 * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            self.dconv_bn_relu(feature_dim * 8, feature_dim * 4),               #(batch, feature_dim * 16, 8, 8)     \n",
    "            self.dconv_bn_relu(feature_dim * 4, feature_dim * 2),               #(batch, feature_dim * 16, 16, 16)     \n",
    "            self.dconv_bn_relu(feature_dim * 2, feature_dim),                   #(batch, feature_dim * 16, 32, 32)     \n",
    "        )\n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2,\n",
    "                               padding=2, output_padding=1, bias=False),\n",
    "            nn.Tanh()   \n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "    def dconv_bn_relu(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2,\n",
    "                               padding=2, output_padding=1, bias=False),        #double height and width\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(y.size(0), -1, 4, 4)\n",
    "        y = self.l2(y)\n",
    "        y = self.l3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHykBwExRr0_"
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (batch, 3, 64, 64)\n",
    "    Output shape: (batch)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, feature_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "            \n",
    "        #input: (batch, 3, 64, 64)\n",
    "        \"\"\"\n",
    "        NOTE FOR SETTING DISCRIMINATOR:\n",
    "\n",
    "        Remove last sigmoid layer for WGAN\n",
    "        \"\"\"\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self.conv_bn_lrelu(feature_dim, feature_dim * 2),                   #(batch, 3, 16, 16)\n",
    "            self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4),               #(batch, 3, 8, 8)\n",
    "            self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8),               #(batch, 3, 4, 4)\n",
    "            nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "    def conv_bn_lrelu(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        NOTE FOR SETTING DISCRIMINATOR:\n",
    "\n",
    "        You can't use nn.Batchnorm for WGAN-GP\n",
    "        Use nn.InstanceNorm2d instead\n",
    "        \"\"\"\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hb7Y38bsR35o"
   },
   "outputs": [],
   "source": [
    "# setting for weight init function\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC-6M2P3SAu9"
   },
   "source": [
    "## Create trainer\n",
    "In this section, we will create a trainer which contains following functions:\n",
    "1. prepare_environment: prepare the overall environment, construct the models, create directory for the log and ckpt\n",
    "2. train: train for generator and discriminator, you can try to modify the code here to construct WGAN or WGAN-GP\n",
    "3. inference: after training, you can pass the generator ckpt path into it and the function will save the result for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8ajFDWBTRzn"
   },
   "outputs": [],
   "source": [
    "class TrainerGAN():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.G = Generator(100)\n",
    "        self.D = Discriminator(3)\n",
    "        \n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "        \"\"\"\n",
    "        NOTE FOR SETTING OPTIMIZER:\n",
    "\n",
    "        GAN: use Adam optimizer\n",
    "        WGAN: use RMSprop optimizer\n",
    "        WGAN-GP: use Adam optimizer \n",
    "        \"\"\"\n",
    "        self.opt_D = torch.optim.Adam(self.D.parameters(), lr=self.config[\"lr\"], betas=(0.5, 0.999))\n",
    "        self.opt_G = torch.optim.Adam(self.G.parameters(), lr=self.config[\"lr\"], betas=(0.5, 0.999))\n",
    "        \n",
    "        self.dataloader = None\n",
    "        self.log_dir = os.path.join(self.config[\"workspace_dir\"], 'logs')\n",
    "        self.ckpt_dir = os.path.join(self.config[\"workspace_dir\"], 'checkpoints')\n",
    "        \n",
    "        FORMAT = '%(asctime)s - %(levelname)s: %(message)s'\n",
    "        logging.basicConfig(level=logging.INFO, \n",
    "                            format=FORMAT,\n",
    "                            datefmt='%Y-%m-%d %H:%M')\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.z_samples = Variable(torch.randn(100, self.config[\"z_dim\"])).cuda()\n",
    "        \n",
    "    def prepare_environment(self):\n",
    "        \"\"\"\n",
    "        Use this funciton to prepare function\n",
    "        \"\"\"\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(self.ckpt_dir, exist_ok=True)\n",
    "        \n",
    "        # update dir by time\n",
    "        time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        self.log_dir = os.path.join(self.log_dir, time+f'_{self.config[\"model_type\"]}')\n",
    "        self.ckpt_dir = os.path.join(self.ckpt_dir, time+f'_{self.config[\"model_type\"]}')\n",
    "        os.makedirs(self.log_dir)\n",
    "        os.makedirs(self.ckpt_dir)\n",
    "        \n",
    "        # create dataset by the above function\n",
    "        dataset = get_dataset(os.path.join(self.config[\"workspace_dir\"], 'faces'))\n",
    "        self.dataloader = DataLoader(dataset, batch_size=self.config[\"batch_size\"], shuffle=True, num_workers=2)\n",
    "        \n",
    "        # model preparation\n",
    "        self.G = self.G.cuda()\n",
    "        self.D = self.D.cuda()\n",
    "        self.G.train()\n",
    "        self.D.train()\n",
    "    def gp(self):\n",
    "        \"\"\"\n",
    "        Implement gradient penalty function\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Use this function to train generator and discriminator\n",
    "        \"\"\"\n",
    "        self.prepare_environment()\n",
    "        \n",
    "        for e, epoch in enumerate(range(self.config[\"n_epoch\"])):\n",
    "            progress_bar = tqdm(self.dataloader)\n",
    "            progress_bar.set_description(f\"Epoch {e+1}\")\n",
    "            for i, data in enumerate(progress_bar):\n",
    "                imgs = data.cuda()\n",
    "                bs = imgs.size(0)\n",
    "\n",
    "                # *********************\n",
    "                # *    Train D        *\n",
    "                # *********************\n",
    "                z = Variable(torch.randn(bs, self.config[\"z_dim\"])).cuda()\n",
    "                r_imgs = Variable(imgs).cuda()\n",
    "                f_imgs = self.G(z)\n",
    "                r_label = torch.ones((bs)).cuda()\n",
    "                f_label = torch.zeros((bs)).cuda()\n",
    "\n",
    "\n",
    "                # Discriminator forwarding\n",
    "                r_logit = self.D(r_imgs)\n",
    "                f_logit = self.D(f_imgs)\n",
    "\n",
    "                \"\"\"\n",
    "                NOTE FOR SETTING DISCRIMINATOR LOSS:\n",
    "                \n",
    "                GAN: \n",
    "                    loss_D = (r_loss + f_loss)/2\n",
    "                WGAN: \n",
    "                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "                WGAN-GP: \n",
    "                    gradient_penalty = self.gp(r_imgs, f_imgs)\n",
    "                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit) + gradient_penalty\n",
    "                \"\"\"\n",
    "                # Loss for discriminator\n",
    "                r_loss = self.loss(r_logit, r_label)\n",
    "                f_loss = self.loss(f_logit, f_label)\n",
    "                loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "                # Discriminator backwarding\n",
    "                self.D.zero_grad()\n",
    "                loss_D.backward()\n",
    "                self.opt_D.step()\n",
    "\n",
    "                \"\"\"\n",
    "                NOTE FOR SETTING WEIGHT CLIP:\n",
    "                \n",
    "                WGAN: below code\n",
    "                \"\"\"\n",
    "                # for p in self.D.parameters():\n",
    "                #     p.data.clamp_(-self.config[\"clip_value\"], self.config[\"clip_value\"])\n",
    "\n",
    "\n",
    "\n",
    "                # *********************\n",
    "                # *    Train G        *\n",
    "                # *********************\n",
    "                if self.steps % self.config[\"n_critic\"] == 0:\n",
    "                    # Generate some fake images.\n",
    "                    z = Variable(torch.randn(bs, self.config[\"z_dim\"])).cuda()\n",
    "                    f_imgs = self.G(z)\n",
    "\n",
    "                    # Generator forwarding\n",
    "                    f_logit = self.D(f_imgs)\n",
    "\n",
    "\n",
    "                    \"\"\"\n",
    "                    NOTE FOR SETTING LOSS FOR GENERATOR:\n",
    "                    \n",
    "                    GAN: loss_G = self.loss(f_logit, r_label)\n",
    "                    WGAN: loss_G = -torch.mean(self.D(f_imgs))\n",
    "                    WGAN-GP: loss_G = -torch.mean(self.D(f_imgs))\n",
    "                    \"\"\"\n",
    "                    # Loss for the generator.\n",
    "                    loss_G = self.loss(f_logit, r_label)\n",
    "\n",
    "                    # Generator backwarding\n",
    "                    self.G.zero_grad()\n",
    "                    loss_G.backward()\n",
    "                    self.opt_G.step()\n",
    "                    \n",
    "                if self.steps % 10 == 0:\n",
    "                    progress_bar.set_postfix(loss_G=loss_G.item(), loss_D=loss_D.item())\n",
    "                self.steps += 1\n",
    "\n",
    "            self.G.eval()\n",
    "            f_imgs_sample = (self.G(self.z_samples).data + 1) / 2.0\n",
    "            filename = os.path.join(self.log_dir, f'Epoch_{epoch+1:03d}.jpg')\n",
    "            torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
    "            logging.info(f'Save some samples to {filename}.')\n",
    "\n",
    "            # Show some images during training.\n",
    "            grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.imshow(grid_img.permute(1, 2, 0))\n",
    "            plt.show()\n",
    "\n",
    "            self.G.train()\n",
    "\n",
    "            if (e+1) % 5 == 0 or e == 0:\n",
    "                # Save the checkpoints.\n",
    "                torch.save(self.G.state_dict(), os.path.join(self.ckpt_dir, f'G_{e}.pth'))\n",
    "                torch.save(self.D.state_dict(), os.path.join(self.ckpt_dir, f'D_{e}.pth'))\n",
    "\n",
    "        logging.info('Finish training')\n",
    "\n",
    "    def inference(self, G_path, n_generate=1000, n_output=30, show=False):\n",
    "        \"\"\"\n",
    "        1. G_path is the path for Generator ckpt\n",
    "        2. You can use this function to generate final answer\n",
    "        \"\"\"\n",
    "\n",
    "        self.G.load_state_dict(torch.load(G_path))\n",
    "        self.G.cuda()\n",
    "        self.G.eval()\n",
    "        z = Variable(torch.randn(n_generate, self.config[\"z_dim\"])).cuda()\n",
    "        imgs = (self.G(z).data + 1) / 2.0\n",
    "        \n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        for i in range(n_generate):\n",
    "            torchvision.utils.save_image(imgs[i], f'output/{i+1}.jpg')\n",
    "        \n",
    "        if show:\n",
    "            row, col = n_output//10 + 1, 10\n",
    "            grid_img = torchvision.utils.make_grid(imgs[:n_output].cpu(), nrow=row)\n",
    "            plt.figure(figsize=(row, col))\n",
    "            plt.imshow(grid_img.permute(1, 2, 0))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uf8BdVoYNJ8"
   },
   "source": [
    "# Train\n",
    "In this section, we will first set the config for trainer, then use it to train generator and discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykjfugCdYmYS"
   },
   "source": [
    "## Set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jg4YdRVPYJSj"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_type\": \"GAN\",\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 1e-4,\n",
    "    \"n_epoch\": 50,\n",
    "    \"n_critic\": 2,\n",
    "    \"z_dim\": 100,\n",
    "    \"workspace_dir\": workspace_dir, # define in the environment setting\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntn56Ffvip-x"
   },
   "source": [
    "## Start to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTHoXrLUYJUn",
    "outputId": "ef81adc1-56f4-4181-e0d1-c1490d7e9266"
   },
   "outputs": [],
   "source": [
    "trainer = TrainerGAN(config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g3_RUzYix0W"
   },
   "source": [
    "# Inference\n",
    "In this section, we will use trainer to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6hdMgj_i3kk"
   },
   "source": [
    "## Inference through trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72EEf52FrOCp"
   },
   "source": [
    "# save the 1000 images into ./output folder\n",
    "trainer.inference(f'{workspace_dir}/checkpoints/2022-03-31_15-59-17_GAN/G_0.pth') # you have to modify the path when running this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuoaEVUgk7oZ"
   },
   "source": [
    "## Prepare .tar file for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QI2cnbbWlA3Z",
    "outputId": "7e2d3e13-250b-4820-b5bb-acaacb6398e4"
   },
   "source": [
    "%cd output\n",
    "!tar -zcf ../submission.tgz *.jpg\n",
    "%cd .."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
